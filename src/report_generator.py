import logging
import os
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple

from scoring import load_scoring_config, apply_scoring_to_posts

logger = logging.getLogger(__name__)

MAX_LINE_MESSAGE_LENGTH = 5000
MAX_TELEGRAM_MESSAGE_LENGTH = 4096
DEFAULT_REPORTS_DIR = "data/reports"

REQUIRED_TOP_KEYS = ["analyzed_posts", "stats", "keywords", "timestamp"]
REQUIRED_POST_KEYS = ["id", "content", "link"]
REQUIRED_ANALYSIS_KEYS = ["categories", "importance", "summary"]


def validate_monitoring_data(data: Dict) -> Tuple[bool, str]:
    """
    é©—è­‰ç›£æ§è³‡æ–™çµæ§‹æ˜¯å¦å®Œæ•´ã€‚

    Args:
        data: ç›£æ§è³‡æ–™å­—å…¸ã€‚

    Returns:
        Tuple[bool, str]: (æ˜¯å¦æœ‰æ•ˆ, éŒ¯èª¤è¨Šæ¯)ã€‚æœ‰æ•ˆæ™‚éŒ¯èª¤è¨Šæ¯ç‚ºç©ºå­—ä¸²ã€‚
    """
    if not isinstance(data, dict):
        return False, "è³‡æ–™å¿…é ˆæ˜¯å­—å…¸æ ¼å¼"

    for key in REQUIRED_TOP_KEYS:
        if key not in data:
            return False, f"ç¼ºå°‘å¿…è¦æ¬„ä½: {key}"

    if not isinstance(data["analyzed_posts"], list):
        return False, "analyzed_posts å¿…é ˆæ˜¯åˆ—è¡¨"

    for i, post in enumerate(data["analyzed_posts"]):
        for key in REQUIRED_POST_KEYS:
            if key not in post:
                return False, f"è²¼æ–‡ #{i} ç¼ºå°‘å¿…è¦æ¬„ä½: {key}"
        if "analysis" not in post:
            return False, f"è²¼æ–‡ #{i} ç¼ºå°‘ analysis æ¬„ä½"
        analysis = post["analysis"]
        for key in REQUIRED_ANALYSIS_KEYS:
            if key not in analysis:
                return False, f"è²¼æ–‡ #{i} çš„ analysis ç¼ºå°‘æ¬„ä½: {key}"

    return True, ""


def classify_posts_by_category(posts: List[Dict]) -> Dict[str, List[Dict]]:
    """
    å°‡å·²åˆ†æçš„è²¼æ–‡ä¾é¡åˆ¥åˆ†çµ„ã€‚ä¸€ç¯‡è²¼æ–‡å¯åŒæ™‚å‡ºç¾åœ¨å¤šå€‹é¡åˆ¥ä¸­ã€‚

    Args:
        posts: å·²åˆ†æçš„è²¼æ–‡åˆ—è¡¨ï¼ˆæ¯ç­†å« analysis æ¬„ä½ï¼‰ã€‚

    Returns:
        Dict[str, List[Dict]]: é¡åˆ¥åç¨± -> è©²é¡åˆ¥çš„è²¼æ–‡åˆ—è¡¨ã€‚
    """
    categorized: Dict[str, List[Dict]] = {}
    for post in posts:
        categories = post.get("analysis", {}).get("categories", [])
        for category in categories:
            if category not in categorized:
                categorized[category] = []
            categorized[category].append(post)
    return categorized


def _get_effective_importance(post: Dict) -> int:
    """å–å¾—è²¼æ–‡çš„æœ‰æ•ˆåˆ†æ•¸ï¼ˆå„ªå…ˆä½¿ç”¨ adjusted_importanceï¼‰ã€‚"""
    analysis = post.get("analysis", {})
    return analysis.get("adjusted_importance", analysis.get("importance", 0))


def identify_big_fish(posts: List[Dict]) -> List[Dict]:
    """
    è­˜åˆ¥å¤§é­šï¼ˆé‡å¤§è­°é¡Œï¼‰è²¼æ–‡ã€‚

    åˆ¤æ–·æ¨™æº–ï¼ˆä½¿ç”¨ adjusted_importanceï¼Œè‹¥ç„¡å‰‡ç”¨åŸå§‹ importanceï¼‰ï¼š
    1. importance >= 9
    2. categories >= 3 ä¸” importance >= 8

    Args:
        posts: å·²åˆ†æçš„è²¼æ–‡åˆ—è¡¨ã€‚

    Returns:
        List[Dict]: å¤§é­šè²¼æ–‡åˆ—è¡¨ï¼ˆä¾æœ‰æ•ˆåˆ†æ•¸é™åºæ’åˆ—ï¼‰ã€‚
    """
    big_fish = []
    for post in posts:
        analysis = post.get("analysis", {})
        importance = _get_effective_importance(post)
        categories = analysis.get("categories", [])

        if importance >= 9:
            big_fish.append(post)
        elif len(categories) >= 3 and importance >= 8:
            big_fish.append(post)

    big_fish.sort(key=_get_effective_importance, reverse=True)
    return big_fish


def compute_category_stats(categorized: Dict[str, List[Dict]]) -> List[Dict]:
    """
    è¨ˆç®—å„é¡åˆ¥çš„çµ±è¨ˆè³‡æ–™ã€‚

    Args:
        categorized: åˆ†é¡å¾Œçš„è²¼æ–‡å­—å…¸ã€‚

    Returns:
        List[Dict]: [{"name": "æ”¿æ²»", "count": 4, "percentage": 40.0}, ...]
                     æŒ‰ count é™åºæ’åˆ—ã€‚
    """
    if not categorized:
        return []

    # è¨ˆç®—å”¯ä¸€è²¼æ–‡æ•¸ï¼ˆç”¨æ–¼ç™¾åˆ†æ¯”åˆ†æ¯ï¼‰
    all_post_ids = set()
    for posts in categorized.values():
        for post in posts:
            all_post_ids.add(post.get("id", ""))
    total_unique = len(all_post_ids) if all_post_ids else 1

    stats = []
    for name, posts in categorized.items():
        count = len(posts)
        percentage = round(count / total_unique * 100, 1)
        stats.append({"name": name, "count": count, "percentage": percentage})

    stats.sort(key=lambda s: s["count"], reverse=True)
    return stats


def generate_markdown_report(data: Dict) -> str:
    """
    å¾ç›£æ§è³‡æ–™ç”Ÿæˆå®Œæ•´çš„ Markdown æ ¼å¼æˆ°å ±ã€‚

    Args:
        data: å®Œæ•´çš„ç›£æ§è³‡æ–™å­—å…¸ã€‚

    Returns:
        str: Markdown æ ¼å¼çš„å®Œæ•´æˆ°å ±ã€‚
    """
    posts = data.get("analyzed_posts", [])
    stats = data.get("stats", {})
    keywords = data.get("keywords", [])
    timestamp = data.get("timestamp", datetime.now().isoformat())

    categorized = classify_posts_by_category(posts)
    big_fish = identify_big_fish(posts)
    category_stats = compute_category_stats(categorized)

    lines = []

    # Header
    lines.append("# Threads è¼¿æƒ…æˆ°å ±")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ™‚é–“**: {timestamp}")
    lines.append(f"**ç›£æ§é—œéµå­—**: {', '.join(keywords)}")
    lines.append(f"**æœ‰æ•ˆè²¼æ–‡æ•¸**: {len(posts)} ç¯‡")
    lines.append("")
    lines.append("---")
    lines.append("")

    # Executive Summary
    lines.append("## åŸ·è¡Œæ‘˜è¦")
    lines.append("")
    big_fish_note = f"ï¼Œç™¼ç¾ {len(big_fish)} å€‹é‡å¤§è­°é¡Œ" if big_fish else ""
    lines.append(
        f"æœ¬æ¬¡ç›£æ§é€±æœŸå…±æƒæ {stats.get('total_searched', 'N/A')} ç­†è²¼æ–‡ï¼Œ"
        f"ç¶“é›™é‡éæ¿¾å¾Œç¯©é¸å‡º {len(posts)} ç­†æœ‰æ•ˆå…§å®¹{big_fish_note}ã€‚"
    )
    lines.append("")

    # Stats table
    lines.append("| é …ç›® | æ•¸é‡ |")
    lines.append("|------|------|")
    lines.append(f"| ç¸½æƒææ•¸ | {stats.get('total_searched', 'N/A')} |")
    lines.append(f"| ç¡¬æ€§éæ¿¾ç§»é™¤ | {stats.get('filtered_by_hard_rules', 'N/A')} |")
    lines.append(f"| å»é‡ç§»é™¤ | {stats.get('filtered_by_dedup', 'N/A')} |")
    lines.append(f"| AI éæ¿¾ç§»é™¤ | {stats.get('filtered_by_ai', 'N/A')} |")
    lines.append(f"| æœ‰æ•ˆè²¼æ–‡ | {stats.get('valid_count', len(posts))} |")
    lines.append("")

    # Category distribution
    if category_stats:
        lines.append("### è­°é¡Œåˆ†å¸ƒ")
        lines.append("")
        lines.append("| é¡åˆ¥ | æ•¸é‡ | ç™¾åˆ†æ¯” |")
        lines.append("|------|------|--------|")
        for cs in category_stats:
            lines.append(f"| {cs['name']} | {cs['count']} | {cs['percentage']}% |")
        lines.append("")

    lines.append("---")
    lines.append("")

    # Big Fish
    if big_fish:
        lines.append("## å¤§é­šè­¦å ±ï¼ˆé‡å¤§è­°é¡Œï¼‰")
        lines.append("")
        for i, fish in enumerate(big_fish, 1):
            analysis = fish.get("analysis", {})
            cats = analysis.get("categories", [])
            cat_label = "][".join(cats)
            eff = _get_effective_importance(fish)
            lines.append(f"### {i}. [{cat_label}] {analysis.get('summary', '')}")
            lines.append("")
            eff_imp = _get_effective_importance(fish)
            base_imp = analysis.get('importance', 'N/A')
            bonus_detail = analysis.get('bonus_detail', [])
            if bonus_detail:
                bonus_str = " + ".join(f"{d['rule_name']}+{d['bonus']}" for d in bonus_detail)
                lines.append(f"- **é‡è¦æ€§**: {eff_imp}/10ï¼ˆåŸå§‹ {base_imp}ï¼ŒåŠ åˆ†: {bonus_str}ï¼‰")
            else:
                lines.append(f"- **é‡è¦æ€§**: {eff_imp}/10")
            lines.append(f"- **ä½œè€…**: @{fish.get('author', 'unknown')}")
            lines.append(f"- **æ™‚é–“**: {fish.get('timestamp', 'N/A')}")
            lines.append(f"- **æ‘˜è¦**: {analysis.get('summary', '')}")

            entities = analysis.get("entities", {})
            if entities:
                if entities.get("persons"):
                    lines.append(f"- **äººç‰©**: {', '.join(entities['persons'])}")
                if entities.get("locations"):
                    lines.append(f"- **åœ°é»**: {', '.join(entities['locations'])}")
                if entities.get("organizations"):
                    lines.append(f"- **çµ„ç¹”**: {', '.join(entities['organizations'])}")
                if entities.get("events"):
                    lines.append(f"- **äº‹ä»¶**: {', '.join(entities['events'])}")

            lines.append(f"- **åŸæ–‡**: {fish.get('link', '')}")
            lines.append(f"- **åˆ†æ**: {analysis.get('reasoning', '')}")
            lines.append("")

        lines.append("---")
        lines.append("")

    # Detailed category sections
    lines.append("## å„é¡åˆ¥è©³æƒ…")
    lines.append("")

    for cs in category_stats:
        cat_name = cs["name"]
        cat_posts = categorized.get(cat_name, [])
        # æŒ‰æœ‰æ•ˆåˆ†æ•¸é™åº
        cat_posts_sorted = sorted(
            cat_posts,
            key=_get_effective_importance,
            reverse=True
        )
        lines.append(f"### {cat_name}ï¼ˆ{len(cat_posts_sorted)} ç¯‡ï¼‰")
        lines.append("")
        for j, post in enumerate(cat_posts_sorted, 1):
            a = post.get("analysis", {})
            imp = _get_effective_importance(post)
            lines.append(f"{j}. [{imp}/10] {a.get('summary', post.get('content', '')[:60])}")
            lines.append(f"   - @{post.get('author', 'unknown')} | "
                         f"[åŸæ–‡]({post.get('link', '')})")
            lines.append("")

    # Footer
    lines.append("---")
    lines.append("")
    lines.append("*å ±å‘Šç”± OpenClaw AI Agent è‡ªå‹•ç”¢ç”Ÿ*")

    return "\n".join(lines)


def generate_line_summary(data: Dict, report_url: Optional[str] = None) -> str:
    """
    å¾ç›£æ§è³‡æ–™ç”Ÿæˆ LINE é€šçŸ¥ç”¨çš„ç²¾ç°¡ç‰ˆæ–‡å­—ï¼ˆå«è²¼æ–‡ URLï¼‰ã€‚

    é™åˆ¶åœ¨ MAX_LINE_MESSAGE_LENGTH å­—å…ƒå…§ã€‚

    Args:
        data: å®Œæ•´çš„ç›£æ§è³‡æ–™å­—å…¸ã€‚
        report_url: å®Œæ•´æˆ°å ±çš„é€£çµï¼ˆå¦‚ Gist URLï¼‰ã€‚

    Returns:
        str: é©åˆ LINE ç™¼é€çš„ç´”æ–‡å­—æ‘˜è¦ï¼ˆå« URLï¼‰ã€‚
    """
    posts = data.get("analyzed_posts", [])
    stats = data.get("stats", {})
    keywords = data.get("keywords", [])
    big_fish = identify_big_fish(posts)

    parts = []

    # Header
    parts.append("ğŸ”” Threads ç›£æ§é€šçŸ¥")
    parts.append(f"ğŸ“Š æƒæ {stats.get('total_searched', 'N/A')} ç­† â†’ æœ‰æ•ˆ {len(posts)} ç­†")
    parts.append(f"ğŸ”‘ é—œéµå­—: {', '.join(keywords)}")
    parts.append("")

    # Big fish
    if big_fish:
        parts.append(f"ğŸŸ å¤§é­šè­¦å ±ï¼ˆ{len(big_fish)} å‰‡ï¼‰:")
        for fish in big_fish:
            a = fish.get("analysis", {})
            eff = _get_effective_importance(fish)
            parts.append(f"[{eff}/10] {a.get('summary', '')}")
            parts.append(f"â†’ {fish.get('link', '')}")
        parts.append("")

    # Other posts (non-big-fish)
    big_fish_ids = {f["id"] for f in big_fish}
    other_posts = [p for p in posts if p["id"] not in big_fish_ids]
    other_posts.sort(
        key=lambda p: p.get("analysis", {}).get("importance", 0), reverse=True
    )

    if other_posts:
        parts.append("ğŸ“‹ å…¶ä»–é‡é»:")
        current_text = "\n".join(parts)

        for post in other_posts:
            a = post.get("analysis", {})
            cats = a.get("categories", [])
            cat_label = "/".join(cats) if cats else "å…¶ä»–"
            entry = f"â€¢ [{cat_label}] {a.get('summary', post.get('content', '')[:40])}"
            url_line = f"  â†’ {post.get('link', '')}"
            candidate = f"{entry}\n{url_line}"

            # æª¢æŸ¥é•·åº¦é™åˆ¶
            if len(current_text) + len(candidate) + 2 > MAX_LINE_MESSAGE_LENGTH - 50:
                parts.append("...æ›´å¤šå…§å®¹è¦‹å®Œæ•´å ±å‘Š")
                break
            parts.append(entry)
            parts.append(url_line)
            current_text = "\n".join(parts)

    # å ±å‘Šé€£çµ
    if report_url:
        parts.append("")
        parts.append(f"ğŸ“„ å®Œæ•´æˆ°å ±: {report_url}")

    return "\n".join(parts)


def generate_telegram_summary(data: Dict, report_url: Optional[str] = None) -> str:
    """
    å¾ç›£æ§è³‡æ–™ç”Ÿæˆ Telegram é€šçŸ¥ç”¨çš„ Markdown æ ¼å¼æ–‡å­—ï¼ˆå«è²¼æ–‡ URLï¼‰ã€‚

    é™åˆ¶åœ¨ MAX_TELEGRAM_MESSAGE_LENGTH å­—å…ƒå…§ã€‚

    Args:
        data: å®Œæ•´çš„ç›£æ§è³‡æ–™å­—å…¸ã€‚
        report_url: å®Œæ•´æˆ°å ±çš„é€£çµï¼ˆå¦‚ Gist URLï¼‰ã€‚

    Returns:
        str: é©åˆ Telegram ç™¼é€çš„ Markdown æ–‡å­—ï¼ˆå« URLï¼‰ã€‚
    """
    posts = data.get("analyzed_posts", [])
    stats = data.get("stats", {})
    keywords = data.get("keywords", [])
    big_fish = identify_big_fish(posts)

    parts = []

    # Header
    parts.append("ğŸ“Š *Threads è¼¿æƒ…æˆ°å ±*")
    parts.append("")
    parts.append(f"æƒæ {stats.get('total_searched', 'N/A')} ç­† â†’ æœ‰æ•ˆ {len(posts)} ç­†")
    parts.append(f"é—œéµå­—: {', '.join(keywords)}")
    parts.append("")

    # Big fish
    if big_fish:
        parts.append(f"ğŸš¨ *ç™¼ç¾ {len(big_fish)} å€‹é‡å¤§è­°é¡Œ*")
        parts.append("")
        for fish in big_fish:
            a = fish.get("analysis", {})
            link = fish.get("link", "")
            summary_text = a.get("summary", "")
            eff = _get_effective_importance(fish)
            parts.append(f"*[{eff}/10]* {summary_text}")
            parts.append(f"[æŸ¥çœ‹åŸæ–‡]({link})")
            parts.append("")

    # Other posts
    big_fish_ids = {f["id"] for f in big_fish}
    other_posts = [p for p in posts if p["id"] not in big_fish_ids]
    other_posts.sort(
        key=lambda p: p.get("analysis", {}).get("importance", 0), reverse=True
    )

    if other_posts:
        parts.append("ğŸ“‹ *å…¶ä»–é‡é»*")
        parts.append("")
        current_text = "\n".join(parts)

        for post in other_posts:
            a = post.get("analysis", {})
            cats = a.get("categories", [])
            cat_label = "/".join(cats) if cats else "å…¶ä»–"
            summary_text = a.get("summary", post.get("content", "")[:40])
            link = post.get("link", "")
            entry = f"â€¢ [{cat_label}] {summary_text} [åŸæ–‡]({link})"

            if len(current_text) + len(entry) + 2 > MAX_TELEGRAM_MESSAGE_LENGTH - 30:
                parts.append("_...æ›´å¤šå…§å®¹è¦‹å®Œæ•´å ±å‘Š_")
                break
            parts.append(entry)
            current_text = "\n".join(parts)

    # å ±å‘Šé€£çµ
    if report_url:
        parts.append("")
        parts.append(f"ğŸ“„ [å®Œæ•´æˆ°å ±]({report_url})")

    return "\n".join(parts)


def upload_to_gist(report_content: str, filename: str = "report.md",
                   description: str = "Threads è¼¿æƒ…æˆ°å ±") -> Optional[str]:
    """
    å°‡æˆ°å ±ä¸Šå‚³åˆ° GitHub Gistï¼Œå›å‚³å…¬é–‹ URLã€‚

    ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ GITHUB_GIST_TOKEN é€²è¡Œèªè­‰ã€‚

    Args:
        report_content: å ±å‘Šå…§å®¹ã€‚
        filename: Gist æª”æ¡ˆåç¨±ã€‚
        description: Gist æè¿°ã€‚

    Returns:
        str æˆ– None: Gist URLï¼Œå¤±æ•—æ™‚å›å‚³ Noneã€‚
    """
    import requests

    token = os.environ.get("GITHUB_GIST_TOKEN")
    if not token:
        logger.error("ç¼ºå°‘ GITHUB_GIST_TOKEN ç’°å¢ƒè®Šæ•¸")
        return None

    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github+json",
    }

    payload = {
        "description": description,
        "public": True,
        "files": {
            filename: {"content": report_content}
        }
    }

    try:
        response = requests.post(
            "https://api.github.com/gists",
            headers=headers,
            json=payload,
            timeout=15
        )
        response.raise_for_status()
        gist_url = response.json().get("html_url", "")
        logger.info("Gist ä¸Šå‚³æˆåŠŸ: %s", gist_url)
        return gist_url

    except requests.exceptions.RequestException as e:
        logger.error("Gist ä¸Šå‚³å¤±æ•—: %s", e)
        return None


def save_report(report_content: str, reports_dir: str = DEFAULT_REPORTS_DIR,
                timestamp: Optional[str] = None) -> str:
    """
    å°‡æˆ°å ±å„²å­˜ç‚º Markdown æª”æ¡ˆã€‚

    Args:
        report_content: Markdown æ ¼å¼çš„æˆ°å ±å…§å®¹ã€‚
        reports_dir: å„²å­˜ç›®éŒ„ã€‚
        timestamp: å¯é¸çš„æ™‚é–“æˆ³ï¼ˆISO 8601ï¼‰ï¼Œé è¨­ä½¿ç”¨ç•¶å‰æ™‚é–“ã€‚

    Returns:
        str: å„²å­˜çš„æª”æ¡ˆè·¯å¾‘ã€‚
    """
    os.makedirs(reports_dir, exist_ok=True)

    if timestamp:
        try:
            dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
        except (ValueError, AttributeError):
            dt = datetime.now()
    else:
        dt = datetime.now()

    filename = f"report_{dt.strftime('%Y%m%d_%H%M%S')}.md"
    filepath = os.path.join(reports_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report_content)

    logger.info("å ±å‘Šå·²å„²å­˜: %s", filepath)
    return filepath


def generate_all_outputs(data: Dict, reports_dir: str = DEFAULT_REPORTS_DIR,
                         upload_gist: bool = False,
                         scoring_config_path: Optional[str] = None
                         ) -> Optional[Dict[str, str]]:
    """
    ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è¼¸å‡ºä¸¦å„²å­˜å ±å‘Šæª”æ¡ˆã€‚å¯é¸ä¸Šå‚³ Gistã€‚
    æœƒè‡ªå‹•å¥—ç”¨ config/scoring.yml çš„åŠ åˆ†è¦å‰‡ã€‚

    Args:
        data: å®Œæ•´çš„ç›£æ§è³‡æ–™å­—å…¸ã€‚
        reports_dir: å ±å‘Šå„²å­˜ç›®éŒ„ã€‚
        upload_gist: æ˜¯å¦ä¸Šå‚³åˆ° GitHub Gistã€‚
        scoring_config_path: è©•åˆ†è¨­å®šæª”è·¯å¾‘ï¼ˆNone ä½¿ç”¨é è¨­ï¼‰ã€‚

    Returns:
        Dict[str, str] æˆ– None: åŒ…å«æ‰€æœ‰è¼¸å‡ºçš„å­—å…¸ï¼Œè³‡æ–™ç„¡æ•ˆæ™‚å›å‚³ Noneã€‚
    """
    valid, error = validate_monitoring_data(data)
    if not valid:
        logger.error("è³‡æ–™é©—è­‰å¤±æ•—: %s", error)
        return None

    # å¥—ç”¨è‡ªè¨‚åŠ åˆ†è¦å‰‡
    if scoring_config_path:
        scoring_config = load_scoring_config(scoring_config_path)
    else:
        scoring_config = load_scoring_config()

    if scoring_config["bonus_rules"]:
        scored_posts = apply_scoring_to_posts(data["analyzed_posts"], scoring_config)
        data = {**data, "analyzed_posts": scored_posts}
        logger.info("å·²å¥—ç”¨ %d æ¢åŠ åˆ†è¦å‰‡", len(scoring_config["bonus_rules"]))

    markdown_report = generate_markdown_report(data)

    report_path = save_report(
        markdown_report,
        reports_dir=reports_dir,
        timestamp=data.get("timestamp")
    )

    # ä¸Šå‚³ Gistï¼ˆå¯é¸ï¼‰
    gist_url = None
    if upload_gist:
        filename = os.path.basename(report_path)
        ts = data.get("timestamp", "")[:10]
        keywords = ", ".join(data.get("keywords", []))
        gist_url = upload_to_gist(
            markdown_report,
            filename=filename,
            description=f"Threads è¼¿æƒ…æˆ°å ± - {keywords} ({ts})"
        )

    line_summary = generate_line_summary(data, report_url=gist_url)
    telegram_summary = generate_telegram_summary(data, report_url=gist_url)

    return {
        "report_path": report_path,
        "markdown_report": markdown_report,
        "line_summary": line_summary,
        "telegram_summary": telegram_summary,
        "gist_url": gist_url,
    }


if __name__ == '__main__':
    import argparse
    import sys

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(
        description="æˆ°å ±ç”Ÿæˆå·¥å…· - å¾ç›£æ§è³‡æ–™ç”Ÿæˆ Markdown å ±å‘Šå’Œé€šçŸ¥æ‘˜è¦"
    )
    parser.add_argument("--input", help="JSON è¼¸å…¥æª”æ¡ˆè·¯å¾‘ï¼ˆçœç•¥å‰‡å¾ stdin è®€å–ï¼‰")
    parser.add_argument("--output-dir", default=DEFAULT_REPORTS_DIR, help="å ±å‘Šè¼¸å‡ºç›®éŒ„")
    parser.add_argument("--format", choices=["all", "markdown", "line", "telegram"],
                        default="all", dest="output_format", help="è¼¸å‡ºæ ¼å¼")
    parser.add_argument("--gist", action="store_true",
                        help="ä¸Šå‚³æˆ°å ±åˆ° GitHub Gist ä¸¦åœ¨æ‘˜è¦ä¸­é™„ä¸Šé€£çµ")
    parser.add_argument("--scoring-config", default=None,
                        help="è©•åˆ†è¨­å®šæª”è·¯å¾‘ï¼ˆé è¨­ config/scoring.ymlï¼‰")

    args = parser.parse_args()

    # Read JSON input
    try:
        if args.input:
            with open(args.input, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = json.load(sys.stdin)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"éŒ¯èª¤: ç„¡æ³•è®€å–è¼¸å…¥è³‡æ–™ - {e}", file=sys.stderr)
        sys.exit(2)

    # Validate
    valid, error = validate_monitoring_data(data)
    if not valid:
        print(f"éŒ¯èª¤: è³‡æ–™é©—è­‰å¤±æ•— - {error}", file=sys.stderr)
        sys.exit(1)

    # å¥—ç”¨åŠ åˆ†è¦å‰‡
    scoring_config = load_scoring_config(args.scoring_config) if args.scoring_config else load_scoring_config()
    if scoring_config["bonus_rules"]:
        scored_posts = apply_scoring_to_posts(data["analyzed_posts"], scoring_config)
        data = {**data, "analyzed_posts": scored_posts}
        logger.info("å·²å¥—ç”¨ %d æ¢åŠ åˆ†è¦å‰‡", len(scoring_config["bonus_rules"]))

    # Generate
    if args.output_format == "line":
        gist_url = None
        if args.gist:
            report = generate_markdown_report(data)
            gist_url = upload_to_gist(report)
        print(generate_line_summary(data, report_url=gist_url))
    elif args.output_format == "telegram":
        gist_url = None
        if args.gist:
            report = generate_markdown_report(data)
            gist_url = upload_to_gist(report)
        print(generate_telegram_summary(data, report_url=gist_url))
    elif args.output_format == "markdown":
        report = generate_markdown_report(data)
        path = save_report(report, reports_dir=args.output_dir,
                           timestamp=data.get("timestamp"))
        print(f"å ±å‘Šå·²å„²å­˜: {path}")
    else:  # all
        outputs = generate_all_outputs(data, reports_dir=args.output_dir,
                                       upload_gist=args.gist,
                                       scoring_config_path=args.scoring_config)
        if outputs:
            print(f"å ±å‘Šå·²å„²å­˜: {outputs['report_path']}")
            if outputs.get("gist_url"):
                print(f"Gist URL: {outputs['gist_url']}")
            print()
            print("=== LINE æ‘˜è¦ ===")
            print(outputs["line_summary"])
            print()
            print("=== Telegram æ‘˜è¦ ===")
            print(outputs["telegram_summary"])
        else:
            print("éŒ¯èª¤: ç”Ÿæˆå ±å‘Šå¤±æ•—", file=sys.stderr)
            sys.exit(1)
